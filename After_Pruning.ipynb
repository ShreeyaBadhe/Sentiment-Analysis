{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636757d1-2800-4858-8e1e-f4427f8033d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Updates: 32.0%, Dev Error: 25.0%\n",
      "Epoch 2, Updates: 30.4%, Dev Error: 25.1%\n",
      "Epoch 3, Updates: 30.3%, Dev Error: 24.4%\n",
      "Epoch 4, Updates: 29.9%, Dev Error: 24.2%\n",
      "Epoch 5, Updates: 30.4%, Dev Error: 24.2%\n",
      "Epoch 6, Updates: 30.7%, Dev Error: 24.5%\n",
      "Epoch 7, Updates: 30.4%, Dev Error: 24.3%\n",
      "Epoch 8, Updates: 30.1%, Dev Error: 24.5%\n",
      "Epoch 9, Updates: 29.9%, Dev Error: 24.7%\n",
      "Epoch 10, Updates: 30.2%, Dev Error: 24.7%\n",
      "Best Dev Error: 24.2%, Time: 14.7 secs\n",
      "Predictions saved to C:/Users/badhe/Downloads/badhe_HW4_ML/test3.predicted.csv\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from collections import Counter\n",
    "\n",
    "# Load word vectors\n",
    "wv = KeyedVectors.load('embs_train.kv')\n",
    "\n",
    "# Function to compute the sentence embedding\n",
    "def sentence_embedding(sentence, word_counts):\n",
    "    tokens = sentence.split()\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        if token in wv and word_counts[token] > 1:  # Consider tokens with sufficient frequency\n",
    "            embeddings.append(wv[token])\n",
    "    if len(embeddings) > 0:\n",
    "        # Return the average embedding\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        # Return a zero vector if no embeddings were found\n",
    "        return np.zeros(wv.vector_size)\n",
    "\n",
    "# Function to process data row by row for training/testing\n",
    "def read_from(dataframe, word_counts):\n",
    "    for _, row in dataframe.iterrows():\n",
    "        label = 1 if row['target'] == '+' else -1  # Convert label to numeric\n",
    "        emb = sentence_embedding(row['sentence'], word_counts)\n",
    "        yield (label, emb)\n",
    "\n",
    "# Test the model\n",
    "def test(data, model, word_counts):\n",
    "    tot, err = 0, 0\n",
    "    for label, emb in read_from(data, word_counts):\n",
    "        err += label * (np.dot(model, emb)) <= 0\n",
    "    return err / len(data)\n",
    "\n",
    "# Train the model using a perceptron-like algorithm\n",
    "def averaged_perceptron(train_data, dev_data, word_counts, epochs=10):\n",
    "    t = time.time()\n",
    "    best_err = 1.0\n",
    "    model = np.zeros(wv.vector_size)  # Initialize model as zero vector\n",
    "    avg_model = np.zeros(wv.vector_size)\n",
    "\n",
    "    for it in range(1, epochs + 1):\n",
    "        updates = 0\n",
    "        for label, emb in read_from(train_data, word_counts):\n",
    "            if label * (np.dot(model, emb)) <= 0:  # Perceptron update rule\n",
    "                updates += 1\n",
    "                model += label * emb\n",
    "            avg_model += model\n",
    "\n",
    "        dev_err = test(dev_data, avg_model, word_counts)\n",
    "        best_err = min(best_err, dev_err)\n",
    "        print(f\"Epoch {it}, Updates: {updates / len(train_data) * 100:.1f}%, Dev Error: {dev_err * 100:.1f}%\")\n",
    "\n",
    "    avg_model /= (epochs * len(train_data))\n",
    "    print(f\"Best Dev Error: {best_err * 100:.1f}%, Time: {time.time() - t:.1f} secs\")\n",
    "    return avg_model\n",
    "\n",
    "# Predict labels for the test set and save to file\n",
    "def predict_test(test_data, model, word_counts, output_file=\"test.predicted.csv\"):\n",
    "    predictions = []\n",
    "    for _, row in test_data.iterrows():\n",
    "        emb = sentence_embedding(row['sentence'], word_counts)\n",
    "        prediction = '+' if np.dot(model, emb) >= 0 else '-'\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    test_data['target'] = predictions\n",
    "    test_data.to_csv(output_file, index=False)\n",
    "    print(f\"Predictions saved to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load training, development, and test data\n",
    "    train_data = pd.read_csv('train.csv')\n",
    "    dev_data = pd.read_csv('dev.csv')\n",
    "    test_data = pd.read_csv('test.csv')\n",
    "\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    for review in train_data['sentence']:\n",
    "        word_counts.update(review.split())\n",
    "\n",
    "    # Train the model\n",
    "    trained_model = averaged_perceptron(train_data, dev_data, word_counts, epochs=10)\n",
    "\n",
    "    # Predict the test set and save the results\n",
    "    predict_test(test_data, trained_model, word_counts, \"C:/Users/badhe/Downloads/badhe_HW4_ML/test3.predicted.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc9f51-9cb7-4fcf-b603-5be97df858ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
